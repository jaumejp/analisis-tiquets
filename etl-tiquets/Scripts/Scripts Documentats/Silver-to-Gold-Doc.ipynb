{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2923f773-efc5-44aa-ab66-d5ece06778e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Explicació del script Silver to Gold\n",
    "- Quin objectiu té la capa Gold en aquest cas?\n",
    "  - L'objectiu principal de la capa Gold és transformar les dades emmagatzemades en la capa Silver en un format i una estructura optimitzats per a l'anàlisi i la presa de decisions. Aquesta capa proporciona les dades preparades per a la creació de gràfics, estadístiques i altres processos.\n",
    "\n",
    "- Quins processos i accions realitzem en aquest script? (EXPLICACIÓ PAS A PAS)\n",
    "\n",
    "  1- Lectura de les dades de la capa Silver:\n",
    "    - Iniciem llegint les dades emmagatzemades en format Parquet a la capa Silver. Utilitzem aquestes dades per crear un DataFrame de Pandas.\n",
    "\n",
    "  2- Selecció de les dades importants:\n",
    "    - Creem diferents funcions per seleccionar les dades específiques que necessitem de la capa Silver. Això pot incloure la filtració de columnes, la transformació de les dades i altres manipulacions necessàries.\n",
    "\n",
    "  3- Estructuració dels DataFrames:\n",
    "    - Creem dos DataFrames amb l'estructura òptima per a les nostres necessitats. \n",
    "\n",
    "  4- Creació de taules a Azure SQL Server:\n",
    "    - En cas que les taules necessàries encara no existeixin a la base de dades Azure SQL Server, les creem en aquest pas. És important tenir les taules preparades abans de penjar les dades per garantir que tinguin l'estructura correcta per a l'emmagatzematge a la capa Gold.\n",
    "\n",
    "  5- Emmagatzematge de les dades a la capa Gold:\n",
    "    - Finalment, emmagatzemem les dades transformades i seleccionades a la capa Gold, que en aquest cas és una base de dades Azure SQL Server. Això ens permet tenir accés a les dades en un format estructurat i optimitzat per a l'anàlisi i la generació de gràfiques.\n",
    "\n",
    "Finalment, emmagatzemem les dades transformades i seleccionades a la capa Gold, que en aquest cas és una base de dades Azure SQL Server. Això ens permet tenir accés a les dades en un format estructurat i optimitzat per a l'anàlisi i la generació de coneixement i conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "972e29e8-9142-4dfb-bbb7-ce34e4f19a68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Importem les llibreries necessàries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a085966-efd4-4f1a-a6b4-4d93bd13c85b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, to_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f640543-d08b-46f5-8528-c9090ba428eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Configuració connexió spark.\n",
    "- En aquest codi podem veure les variables que necessitem per realitzar la connexió amb spark. I com creem l'objecta spark amb aquestes variables.\n",
    "- La connexió en aquest cas ho fem amb SAS, ja que només ens fa falta un token per poder-la establir. Hi ha més maneras que és pot realitzar la connexió."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa1b8c60-ec47-4adf-aed9-a1b308ba2b63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuració Spark per connexió ADLG2 amb token SAS\n",
    "token = 'sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2025-06-03T02:34:04Z&st=2024-04-09T18:34:04Z&spr=https&sig=dplYYYNSRC%2FjDR89WMxcsA6SMo%2BHFYrQ5BqdhRUKBMs%3D'\n",
    "storage_account = 'projecteiabd'\n",
    "container = 'silver'\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.{0}.dfs.core.windows.net\".format(storage_account), \"SAS\")\n",
    "spark.conf.set(\"fs.azure.sas.token.provider.type.{0}.dfs.core.windows.net\".format(storage_account), \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.sas.fixed.token.{0}.dfs.core.windows.net\".format(storage_account), token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c52ccd0c-e63d-4f8c-b511-2b94e6f82585",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Declaració de funcions.\n",
    "A continuació declararem les funcións que utilitzarem per tractar i penjar les dades a la capa Gold.\n",
    "\n",
    "- **Funció per llegir tots els fitxer \".parquet\" de la partició que li assignem.**\n",
    "  - Aquesta funció ens permet llegir els fitxers .parquet del Azure Data Lake de una particó en concret.\n",
    "  - Ens retorna totes les dades dins de un Data Frame.\n",
    "\n",
    "- **Funció per llegir tiquets d'un client concret**.\n",
    "  - Aquesta funció ens permet llegir la informació dels tiquets del client que l'indiquem.\n",
    "  - Retorna un Data Frame amb tota la informació dels tiquets del clients.\n",
    "\n",
    "- **Funció per llegir tiquets d'un día concret**\n",
    "  - Aquesta funció ens permet llegir la informació dels tiquets de un día concret.\n",
    "  - Retorna un Data Frame amb tota la informació dels tiquets del día concret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02f51e76-172d-4674-9f0f-b1c7fda34214",
     "showTitle": false,
     "title": "Funció per llegir tots els .parquet de la partició que ens interessa."
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Funció per llegir tots els fitxer \".parquet\" de la partició que li assignem.\n",
    "\n",
    "def read_all_silver(particio, cotainer, storage_account):\n",
    "     docs = []\n",
    "\n",
    "     # Li diguem la carpeta on tenim les particons. I ja els ha llegeix totes automàticament, no fa falta la funció recursiva.\n",
    "     path = 'abfs://{0}@{1}.dfs.core.windows.net/{2}'.format(container, storage_account,particio)\n",
    "     \n",
    "     df = spark.read.parquet(path)\n",
    "     \n",
    "     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dea86a66-5f06-4aff-b072-7dca18d140a7",
     "showTitle": false,
     "title": "Funció per llegir tiquets d'un client en concret."
    }
   },
   "outputs": [],
   "source": [
    "# Funció per llegir tiquets d'un client concret\n",
    "def read_ticket_client(id_client, cotainer, storage_account):\n",
    "     docs = []\n",
    "\n",
    "     # Li diguem la carpeta on tenim les particons. I ja els ha llegeix totes automàticament, no fa falta la funció recursiva.\n",
    "     path = 'abfs://{0}@{1}.dfs.core.windows.net/clients/id_client={2}'.format(container, storage_account, id_client)\n",
    "     \n",
    "     df = spark.read.parquet(path)\n",
    "     \n",
    "     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46ccc68c-9d66-42ab-8e76-2636ce23120a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Cridem la funció anterior per mostrar un exemple del funcionament.\n",
    "# df_tiquet = read_ticket_client('1', container, storage_account)\n",
    "\n",
    "# # Convertim el resultat en un df de pandas.\n",
    "# df_tiquet.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c81b3be-7c27-4a11-99bd-52fb30eb9e1d",
     "showTitle": false,
     "title": "Funció per llegir tiquets d'un día concret"
    }
   },
   "outputs": [],
   "source": [
    "# Funció per llegir tiquets d'un día concret\n",
    "def read_ticket_data(data, cotainer, storage_account):\n",
    "     docs = []\n",
    "\n",
    "     # Li diguem la carpeta on tenim les particons. I ja els ha llegeix totes automàticament, no fa falta la funció recursiva.\n",
    "     path = 'abfs://{0}@{1}.dfs.core.windows.net/dies/data={2}'.format(container, storage_account, data)\n",
    "     \n",
    "     df = spark.read.parquet(path)\n",
    "     \n",
    "     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79906f3-1acf-4663-98d4-9a18cae4d016",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ciutat</th>\n",
       "      <th>factura_simplificada</th>\n",
       "      <th>products</th>\n",
       "      <th>total_amb_IVA</th>\n",
       "      <th>id_tiquet</th>\n",
       "      <th>id_client</th>\n",
       "      <th>data_scanner_tiquet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BADALONA</td>\n",
       "      <td>1234-123-12345</td>\n",
       "      <td>[{'descripcio': 'LASANYA', 'import': '17.60', ...</td>\n",
       "      <td>17,60</td>\n",
       "      <td>24052024_173502</td>\n",
       "      <td>1</td>\n",
       "      <td>24-05-2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ciutat</th>\n      <th>factura_simplificada</th>\n      <th>products</th>\n      <th>total_amb_IVA</th>\n      <th>id_tiquet</th>\n      <th>id_client</th>\n      <th>data_scanner_tiquet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BADALONA</td>\n      <td>1234-123-12345</td>\n      <td>[{'descripcio': 'LASANYA', 'import': '17.60', ...</td>\n      <td>17,60</td>\n      <td>24052024_173502</td>\n      <td>1</td>\n      <td>24-05-2024</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cridem la funció anterior per mostrar un exemple del funcionament.\n",
    "df_tiquet_dies = read_ticket_data('22-04-2024', container, storage_account)\n",
    "\n",
    "# Convertim el resultat en un df de pandas.\n",
    "df_tiquet_dies.toPandas() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396e4435-c618-4ff0-be40-06efd43e1e5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Tractament de dades per penjar a SQL SERVER\n",
    "- En aquest tractarem el df, per penjar-ho a la capa Gold que és un Azure SQL Server. \n",
    "- Per fer-ho creearem dos df amb l'estructura i dades que volem tenir a la taula recional SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be78abd-c1c2-40a6-b60f-d75b687550ce",
     "showTitle": false,
     "title": "Tractar i penjar a taula racional SQL"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ciutat</th>\n",
       "      <th>data</th>\n",
       "      <th>factura_simplificada</th>\n",
       "      <th>products</th>\n",
       "      <th>total_amb_IVA</th>\n",
       "      <th>id_tiquet</th>\n",
       "      <th>data_scanner_tiquet</th>\n",
       "      <th>id_client</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BADALONA</td>\n",
       "      <td>22-04-2024</td>\n",
       "      <td>1234-123-12345</td>\n",
       "      <td>[{'descripcio': 'LASANYA', 'import': '17.60', ...</td>\n",
       "      <td>17,60</td>\n",
       "      <td>24052024_173502</td>\n",
       "      <td>24-05-2024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ciutat</th>\n      <th>data</th>\n      <th>factura_simplificada</th>\n      <th>products</th>\n      <th>total_amb_IVA</th>\n      <th>id_tiquet</th>\n      <th>data_scanner_tiquet</th>\n      <th>id_client</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BADALONA</td>\n      <td>22-04-2024</td>\n      <td>1234-123-12345</td>\n      <td>[{'descripcio': 'LASANYA', 'import': '17.60', ...</td>\n      <td>17,60</td>\n      <td>24052024_173502</td>\n      <td>24-05-2024</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# En aquest cas per passar a la capa gold(SQL) agafem totes les dades de una partició sense importar quina sigui ja que de aquesta forma penjarem totes les dades a la capa Gold. \n",
    "# Que es el que ens interessa poder mostrar estadístiques i gràfiques de totes les dades en aquest cas.\n",
    "\n",
    "# Llegim tots els parquets, amb la funció per llegir-ho tot de totes les particións.\n",
    "df_parquet = read_all_silver('clients', container, storage_account)\n",
    "\n",
    "# Convertim el resultat en un df de pandas.\n",
    "df_parquet = df_parquet.toPandas() \n",
    "df_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7131be21-b5f8-4991-a41f-fedf05f0556c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Per mostrar-ho a power bi crearem els següents df que posteriorment penjarem a sql.\n",
    "# tiquets(id_tiquet(CP), id_client, total_amb_IVA, data_scanner_tiquet, ciutat)\n",
    "# productes(name, qty, format, p_u_amb_iva, preu_amb_iva, id_tiquet (CF))\n",
    "product_data = []\n",
    "tiquets_data = []\n",
    "\n",
    "# Itearem sobre cada fila del df.\n",
    "for index, row in df_parquet.iterrows():\n",
    "    # Creearem el df de tiquet posant les dades que ens interessent.\n",
    "    tiquets_data.append({\n",
    "        'id_tiquet': row['id_tiquet'],\n",
    "        'id_client': row['id_client'],\n",
    "        'total_amb_IVA': row['total_amb_IVA'],\n",
    "        'data_scanner_tiquet': row['data_scanner_tiquet'],\n",
    "        'data_tiquet': row['data'],\n",
    "        'ciutat': row['ciutat'],\n",
    "    })\n",
    "    # Per cada fila iterem per els productes.\n",
    "    for producte in row['products']:\n",
    "        # Creem una nova fila per producte i li associem valors de la fila. Com l'id tiquet... Que ens servirant per el Power Bi.\n",
    "        # Anem afegint les dades / files a la llista.\n",
    "        product_data.append({\n",
    "            'descripcio': producte['descripcio'],\n",
    "            'quantitat': producte['quantitat'],\n",
    "            'preu unitari': producte['preu_unitari'],\n",
    "            'import': producte['import'],\n",
    "            'unitat': producte['unitat'],\n",
    "            'id_tiquet': row['id_tiquet']\n",
    "        })\n",
    "\n",
    "# Converitm l'array de objectes amb un dataframe.\n",
    "df_productes_bi = pd.DataFrame(product_data)\n",
    "df_tiquets_bi = pd.DataFrame(tiquets_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46467529-71b9-49c7-8c64-b9ecff26b775",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creació de l'estructura SQL\n",
    "Abans de penjar les dades a les taules SQL, és crucial tenir les taules creades a la nostra base de dades SQL.\n",
    "\n",
    "**Com hem definit l'estructura?**\n",
    "  - Hem optat per dividir les dades en dues taules per facilitar l'anàlisi i la visualització. Això ens permetrà realitzar estadístiques i gràfics amb més facilitat, ja que evita la duplicació de dades i ens permet incorporar més informació.\n",
    "  - Hem separat els camps que pertanyen als tiquets en general i els camps que descriuen les línies de productes dels tiquets.\n",
    "  - Les dues taules estan relacionades pel camp 'id_tiquet', que actua com a clau forana a la taula 'lin_productes'. Hem afegit una clau primària ('PK') a la taula 'lin_productes' per identificar de manera única les línies dels tiquets.\n",
    "\n",
    "**ESTRUCTURA DE LA BASE DE DADES**\n",
    "\n",
    "**Taula: tiquets**\n",
    "- Camps:\n",
    "  - ciutat\n",
    "  - data_scanner_tiquet\n",
    "  - data_tiquet\n",
    "  - id_client\n",
    "  - id_tiquet (PK)\n",
    "  - total_amb_IVA\n",
    "\n",
    "\n",
    "**Taula: lin_productes**\n",
    "- Camps:\n",
    "  - descripcio\n",
    "  - id_tiquet (FK)\n",
    "  - import\n",
    "  - preu_unitari\n",
    "  - quanitat\n",
    "  - unitat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5801ca2d-0b1f-4152-95e4-5b8e043e52ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Penjem les dades al servidor Azure SQL Server\n",
    "Un cop tenim el següent preparat:\n",
    "  - Les taules amb l'estructura necessària al servidor SQL.\n",
    "  - Els DataFrame (DF) creats que volem penjar a les taules amb la mateixa estructura, ja que sinó no ens permetrà fer-ho.\n",
    "\n",
    "Per a això, ens connectarem al SQL Server mitjançant el JDBC de SQL. Utilitzarem les credencials i dades necessàries (que obtenim d'Azure).\n",
    "\n",
    "Un cop establerta la connexió, només caldrà escriure als DataFrame corresponents les taules:\n",
    "  - Primer, convertirem els DF de Pandas a DF de Spark.\n",
    "  - A continuació, posarem el mode 'overwrite' per sobrescriure el contingut i evitar problemes amb claus primàries duplicades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988b7cee-5d69-48f8-87d3-2599bc4f6532",
     "showTitle": false,
     "title": "Penjem les dades al servidor Azure SQL Server"
    }
   },
   "outputs": [],
   "source": [
    "# Cadena de connexió JDBC (llibreria de connexió)\n",
    "jdbc_url = \"jdbc:sqlserver://sql-otorrent-etl.database.windows.net:1433;database=sqldb-otorrent-etl;\"\n",
    "\n",
    "# Propietats de connexió (Dades que utiltizem per realitzar les connexións)\n",
    "connection_properties = {\n",
    "    \"user\": \"oriol@sql-otorrent-etl\",\n",
    "    \"password\": \"41598051KKk\",\n",
    "    \"encrypt\": \"true\",\n",
    "    \"trustServerCertificate\": \"false\",\n",
    "    \"hostNameInCertificate\": \"*.database.windows.net\",\n",
    "    \"loginTimeout\": \"30\"\n",
    "}\n",
    "\n",
    "# Panjem els dfs al sql server.\n",
    "spark.createDataFrame(df_tiquets_bi).write.jdbc(url=jdbc_url, table=\"dbo.tiquets\", mode=\"overwrite\", properties=connection_properties)\n",
    "spark.createDataFrame(df_productes_bi).write.jdbc(url=jdbc_url, table=\"dbo.lin_prodcutes\", mode=\"overwrite\", properties=connection_properties)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver-to-Gold-Doc",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
