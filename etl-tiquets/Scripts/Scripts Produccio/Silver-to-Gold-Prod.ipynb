{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2923f773-efc5-44aa-ab66-d5ece06778e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Script Silver to Gold Producció.\n",
    "- Executa les comandes mínimes però necessaries per realitzar el procès més optim possible.\n",
    "- Per veure com s'ha arribat a desenvolupar aquest df i que fa cada pas mirar el Notebook de documentació."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1957387-3919-48d7-a612-56dececcb45a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Importem les llibreries necessàries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f918bb8-e129-4f62-b32f-fc7e89c8f814",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, to_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f640543-d08b-46f5-8528-c9090ba428eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Configuració connexió spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa1b8c60-ec47-4adf-aed9-a1b308ba2b63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "token = 'sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2025-06-03T02:34:04Z&st=2024-04-09T18:34:04Z&spr=https&sig=dplYYYNSRC%2FjDR89WMxcsA6SMo%2BHFYrQ5BqdhRUKBMs%3D'\n",
    "storage_account = 'projecteiabd'\n",
    "container = 'silver'\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.{0}.dfs.core.windows.net\".format(storage_account), \"SAS\")\n",
    "spark.conf.set(\"fs.azure.sas.token.provider.type.{0}.dfs.core.windows.net\".format(storage_account), \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.sas.fixed.token.{0}.dfs.core.windows.net\".format(storage_account), token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c52ccd0c-e63d-4f8c-b511-2b94e6f82585",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Declaració de funcions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02f51e76-172d-4674-9f0f-b1c7fda34214",
     "showTitle": false,
     "title": "Funció per llegir tots els .parquet de la partició que ens interessa."
    }
   },
   "outputs": [],
   "source": [
    "# Funció per llegir tots els fitxer \".parquet\" de la partició que li assignem.\n",
    "import pandas as pd\n",
    "\n",
    "def read_all_silver(particio, cotainer, storage_account):\n",
    "     docs = []\n",
    "\n",
    "     # Li diguem la carpeta on tenim les particons. I ja els ha llegeix totes automàticament, no fa falta la funció recursiva.\n",
    "     path = 'abfs://{0}@{1}.dfs.core.windows.net/{2}'.format(container, storage_account,particio)\n",
    "     \n",
    "     df = spark.read.parquet(path)\n",
    "     \n",
    "     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dea86a66-5f06-4aff-b072-7dca18d140a7",
     "showTitle": false,
     "title": "Funció per llegir tiquets d'un client en concret."
    }
   },
   "outputs": [],
   "source": [
    "# Funció per llegir tiquets d'un client concret\n",
    "def read_ticket_client(id_client, cotainer, storage_account):\n",
    "     docs = []\n",
    "\n",
    "     # Li diguem la carpeta on tenim les particons. I ja els ha llegeix totes automàticament, no fa falta la funció recursiva.\n",
    "     path = 'abfs://{0}@{1}.dfs.core.windows.net/clients/id_client={2}'.format(container, storage_account, id_client)\n",
    "     \n",
    "     df = spark.read.parquet(path)\n",
    "     \n",
    "     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c81b3be-7c27-4a11-99bd-52fb30eb9e1d",
     "showTitle": false,
     "title": "Funció per llegir tiquets d'un día concret"
    }
   },
   "outputs": [],
   "source": [
    "# Funció per llegir tiquets d'un día concret\n",
    "def read_ticket_data(data, cotainer, storage_account):\n",
    "     docs = []\n",
    "\n",
    "     # Li diguem la carpeta on tenim les particons. I ja els ha llegeix totes automàticament, no fa falta la funció recursiva.\n",
    "     path = 'abfs://{0}@{1}.dfs.core.windows.net/dies/data={2}'.format(container, storage_account, data)\n",
    "     \n",
    "     df = spark.read.parquet(path)\n",
    "     \n",
    "     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396e4435-c618-4ff0-be40-06efd43e1e5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Tractament de dades per penjar a SQL SERVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be78abd-c1c2-40a6-b60f-d75b687550ce",
     "showTitle": false,
     "title": "Tractar i penjar a taula racional SQL"
    }
   },
   "outputs": [],
   "source": [
    "# En aquest cas per passar a la capa gold(SQL) agafem totes les dades de una partició sense importar quina sigui ja que de aquesta forma penjarem totes les dades a la capa Gold. \n",
    "# Que es el que ens interessa poder mostrar estadístiques i gràfiques de totes les dades en aquest cas.\n",
    "\n",
    "# Llegim tots els parquets, amb la funció per llegir-ho tot de totes les particións.\n",
    "df_parquet = read_all_silver('clients', container, storage_account)\n",
    "\n",
    "# Convertim el resultat en un df de pandas.\n",
    "df_parquet = df_parquet.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7131be21-b5f8-4991-a41f-fedf05f0556c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "product_data = []\n",
    "tiquets_data = []\n",
    "\n",
    "for index, row in df_parquet.iterrows():\n",
    "    tiquets_data.append({\n",
    "        'id_tiquet': row['id_tiquet'],\n",
    "        'id_client': row['id_client'],\n",
    "        'total_amb_IVA': row['total_amb_IVA'],\n",
    "        'data_scanner_tiquet': row['data_scanner_tiquet'],\n",
    "        'data_tiquet': row['data'],\n",
    "        'ciutat': row['ciutat'],\n",
    "    })\n",
    "\n",
    "    for producte in row['products']:\n",
    "        product_data.append({\n",
    "            'descripcio': producte['descripcio'],\n",
    "            'quantitat': producte['quantitat'],\n",
    "            'preu unitari': producte['preu_unitari'],\n",
    "            'import': producte['import'],\n",
    "            'unitat': producte['unitat'],\n",
    "            'id_tiquet': row['id_tiquet']\n",
    "        })\n",
    "\n",
    "# Converitm l'array de objectes amb un dataframe.\n",
    "df_productes_bi = pd.DataFrame(product_data)\n",
    "df_tiquets_bi = pd.DataFrame(tiquets_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5801ca2d-0b1f-4152-95e4-5b8e043e52ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Penjem les dades al servidor Azure SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988b7cee-5d69-48f8-87d3-2599bc4f6532",
     "showTitle": false,
     "title": "Penjem les dades al servidor Azure SQL Server"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_json\n",
    "\n",
    "\n",
    "# Cadena de connexió JDBC (llibreria de connexió)\n",
    "jdbc_url = \"jdbc:sqlserver://sql-otorrent-etl.database.windows.net:1433;database=sqldb-otorrent-etl;\"\n",
    "\n",
    "# Propietats de connexió (Dades que utiltizem per realitzar les connexións)\n",
    "connection_properties = {\n",
    "    \"user\": \"oriol@sql-otorrent-etl\",\n",
    "    \"password\": \"41598051KKk\",\n",
    "    \"encrypt\": \"true\",\n",
    "    \"trustServerCertificate\": \"false\",\n",
    "    \"hostNameInCertificate\": \"*.database.windows.net\",\n",
    "    \"loginTimeout\": \"30\"\n",
    "}\n",
    "\n",
    "# Panjem els dfs al sql server.\n",
    "spark.createDataFrame(df_tiquets_bi).write.jdbc(url=jdbc_url, table=\"dbo.tiquets\", mode=\"overwrite\", properties=connection_properties)\n",
    "spark.createDataFrame(df_productes_bi).write.jdbc(url=jdbc_url, table=\"dbo.lin_prodcutes\", mode=\"overwrite\", properties=connection_properties)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver-to-Gold-Prod",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
